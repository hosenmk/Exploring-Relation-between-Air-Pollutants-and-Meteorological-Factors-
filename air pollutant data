import pandas as pd
import numpy as np
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import norm
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import ElasticNet
from sklearn.cross_decomposition import PLSRegression
from sklearn.decomposition import PCA
from sklearn.metrics import r2_score

# Step 1: Load and prepare your dataset
# Assuming you have a CSV file named 'data.csv'
data = pd.read_csv('AQI Compiled Data csv.csv')
data.head()

# Splitting into independent variables (X) and dependent variables (Y)
X = data[['Precipitation (mm/d)', 'Specific Humidity (Mass fraction)',
       'Temperature (k)', 'Wind_f_inst (m/s)', 'Air Pressure (Pa)']]
Y = data[['CO column number density (micromole/m^2)', 'NO2 column number density (micromole/m^2)',
       'SO2 column number density (micromole/m^2)',
       'O3 column number density (micromole/m^2)', 'Optical Depth 047']]

# Plot histograms for each variable
plt.figure(figsize=(10, 6))
plt.hist(X['Precipitation (mm/d)'], bins=30, alpha=0.5, label='Precipitation')
plt.title('Histogram of a Single Variable')
plt.xlabel('Values')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

# Plot histograms for each variable
plt.figure(figsize=(10, 6))
plt.hist(X['Specific Humidity (Mass fraction)'], bins=30, alpha=0.5, label='Specific Humidity (Mass fraction)')
plt.title('Histogram of Specific Humidity')
plt.xlabel('Values')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()


# Plot histograms for each variable
plt.figure(figsize=(10, 6))
plt.hist(X['Temperature (k)'], bins=30, alpha=0.5, label='Temperature (k)')
plt.title('Histogram of Temperature (k)')
plt.xlabel('Values')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

# Plot histograms for each variable
plt.figure(figsize=(10, 6))
plt.hist(X['Wind_f_inst (m/s)'], bins=30, alpha=0.5, label='Wind_f_inst (m/s)')
plt.title('Histogram of Wind_f_inst (m/s)')
plt.xlabel('Values')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

# Plot histograms for each variable
plt.figure(figsize=(10, 6))
plt.hist(X['Air Pressure (Pa)'], bins=30, alpha=0.5, label='Air Pressure (Pa)')
plt.title('Histogram of a Air Pressure (Pa)')
plt.xlabel('Values')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

# Perform Shapiro-Wilk test
statistic, p_value = stats.shapiro(X['Precipitation (mm/d)'])

# Print the results
print("Shapiro-Wilk Test:")
print(f"Test Statistic: {statistic}")
print(f"P-value: {p_value}")

# Interpret the results
alpha = 0.05
if p_value > alpha:
    print("The sample looks Gaussian (fail to reject H0)")
else:
    print("The sample does not look Gaussian (reject H0)")


# Perform Shapiro-Wilk test
statistic, p_value = stats.shapiro(X['Specific Humidity (Mass fraction)'])

# Print the results
print("Shapiro-Wilk Test:")
print(f"Test Statistic: {statistic}")
print(f"P-value: {p_value}")

# Interpret the results
alpha = 0.05
if p_value > alpha:
    print("The sample looks Gaussian (fail to reject H0)")
else:
    print("The sample does not look Gaussian (reject H0)")


# Perform Shapiro-Wilk test
statistic, p_value = stats.shapiro(X['Temperature (k)'])

# Print the results
print("Shapiro-Wilk Test:")
print(f"Test Statistic: {statistic}")
print(f"P-value: {p_value}")

# Interpret the results
alpha = 0.05
if p_value > alpha:
    print("The sample looks Gaussian (fail to reject H0)")
else:
    print("The sample does not look Gaussian (reject H0)")


# Perform Shapiro-Wilk test
statistic, p_value = stats.shapiro(X['Wind_f_inst (m/s)'])

# Print the results
print("Shapiro-Wilk Test:")
print(f"Test Statistic: {statistic}")
print(f"P-value: {p_value}")

# Interpret the results
alpha = 0.05
if p_value > alpha:
    print("The sample looks Gaussian (fail to reject H0)")
else:
   print("The sample does not look Gaussian (reject H0)")

# Perform Shapiro-Wilk test
statistic, p_value = stats.shapiro(X['Air Pressure (Pa)'])

# Print the results
print("Shapiro-Wilk Test:")
print(f"Test Statistic: {statistic}")
print(f"P-value: {p_value}")

# Interpret the results
alpha = 0.05
if p_value > alpha:
    print("The sample looks Gaussian (fail to reject H0)")
else:
    print("The sample does not look Gaussian (reject H0)")


correlation_matrix = X.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

# Step 2: Split the dataset into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Perform multivariate multiple linear regression separately for each dependent variable
for col in Y_train.columns:
model = sm.OLS(Y_train[col], sm.add_constant(X_train)).fit()
print(f"Dependent Variable: {col}")
print(model.summary())
print("")


# Assuming X_train and Y_train are your training data for independent and dependent variables respectively

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

# Set the alpha value for Ridge regression
alpha = 1.0  # You can adjust this parameter based on your requirement

for col in Y_train.columns:
# Fit the Ridge regression model
ridge_model = Ridge(alpha=alpha)
ridge_model.fit(X_train_scaled, Y_train[col])

# Predictions
y_pred = ridge_model.predict(X_train_scaled)

# Calculate R-squared
r2 = r2_score(Y_train[col], y_pred)

print(f"Dependent Variable: {col}")
print("Intercept:", ridge_model.intercept_)
print("Coefficients:", ridge_model.coef_)
print("R-squared:", r2)
print("")

# Calculate R-squared
# Predictions
y_pred = ridge_model.predict(X_train_scaled)
r2 = r2_score(Y_train, y_pred)
print("R-squared:", r2)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

# Set the alpha value for ElasticNet regression
alpha = 1.0  # You can adjust this parameter based on your requirement
l1_ratio = 0.5  # Mixing parameter that controls the balance between L1 and L2 penalties

for col in Y_train.columns:
# Fit the ElasticNet regression model
elasticnet_model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)
elasticnet_model.fit(X_train_scaled, Y_train[col])

print(f"Dependent Variable: {col}")
print("Intercept:", elasticnet_model.intercept_)
print("Coefficients:", elasticnet_model.coef_)
print("")
print(model_summary)


# Define ElasticNet model
alpha = 0.5  # Regularization strength
l1_ratio = 0.5  # Ratio of L1 to L2 regularization (1 for Lasso, 0 for Ridge)
elastic_net = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)

y_train_CO = Y_train['CO column number density (micromole/m^2)']
# Fit the model
elastic_net.fit(X_train, y_train_CO)

# Make predictions
y_pred = elastic_net.predict(X_test)

# Calculate Mean Squared Error
mse = mean_squared_error(Y_test['CO column number density (micromole/m^2)'], y_pred)
print("Mean Squared Error:", mse)

# Model Summary
model_summary = pd.DataFrame({
    'Feature': ['Intercept'] + [f'X{i+1}' for i in range(5)],
    'Coefficient': [elastic_net.intercept_] + list(elastic_net.coef_)
})
print("\nModel Summary:")
print(model_summary)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Fit PLS regression model
n_components = 3  # Number of components to keep
pls = PLSRegression(n_components=n_components)
pls.fit(X_train_scaled, Y_train)

# Make predictions
Y_pred = pls.predict(X_test_scaled)

# Calculate Mean Squared Error
mse = mean_squared_error(Y_test, Y_pred)

# Calculate R-squared
r_squared = r2_score(Y_test, Y_pred)

# Calculate F1 Score (You need to specify the exact way you want to calculate F1 score as it's typically used for classification tasks)
# Here, we'll just set it to 0 as we're dealing with regression
f1 = 0

# PLS model summary
coefficients = pls.coef_
model_summary = pd.DataFrame(coefficients, columns=Y.columns, index=X.columns)
model_summary['Mean Squared Error'] = mse
model_summary['R-squared'] = r_squared
model_summary['F1 Score'] = f1

print("\nModel Summary:")
print(model_summary)

# Define the number of components for PLS regression (experiment with different values)
n_components = 5

# Create a PLS regression object
pls = PLSRegression(n_components=n_components)

# Fit the model on the training data (assuming you have already split into training and testing sets)
pls.fit(X_train, Y_train)

# Get coefficients for each dependent variable
coefficients = pls.coef_

# Print the coefficients in a pandas DataFrame for better organization
coefficient_df = pd.DataFrame(coefficients.T, columns=Y.columns, index=X.columns)
print("Coefficients:")
print(coefficient_df)

# Predict on the testing data (assuming you have a testing set)
y_pred = pls.predict(X_test)

# Calculate R-squared score for each dependent variable
r2_scores = []
for i in range(Y.shape[1]):
  r2_scores.append(r2_score(Y_test.iloc[:, i], y_pred[:, i]))

# Print R-squared scores in a pandas Series for easy access
r2_series = pd.Series(r2_scores, index=Y.columns)
print("R-squared Values:")
print(r2_series)

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Applying PCA
pca = PCA()
X_pca = pca.fit_transform(X_scaled)

# Scree plot
plt.figure(figsize=(10, 6))
plt.plot(range(1, pca.n_components_ + 1), pca.explained_variance_ratio_, marker='o', linestyle='-')
plt.title('Scree Plot')
plt.xlabel('Number of Components')
plt.ylabel('Explained Variance Ratio')
plt.xticks(np.arange(1, pca.n_components_ + 1))
plt.grid(True)
plt.show()

# Apply Box-Cox transformation
# Load your data into a pandas DataFrame (assuming it's stored in X)
# Replace 'Precipitation (mm/d)' with the actual name of your column
X['Precipitation (mm/d)'] += 1  # Add 1 to shift the data, making it positive

# Apply Box-Cox transformation
transformed_data_precip, lambda_value = stats.boxcox(X['Precipitation (mm/d)'])

# Plot original and transformed data
fig, axs = plt.subplots(1, 2, figsize=(12, 5))

# Original data histogram
axs[0].hist(X['Precipitation (mm/d)'], bins=20, color='skyblue', edgecolor='black', alpha=0.7)
axs[0].set_title('Original Data')

# Transformed data histogram
axs[1].hist(transformed_data_precip, bins=20, color='lightgreen', edgecolor='black', alpha=0.7)
axs[1].set_title('Box-Cox Transformed Data')

plt.tight_layout()
plt.show()

print("Lambda value:", lambda_value)

